#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Ä–µ–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã AppExchange —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
"""

import requests
import time
import json
import re
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

def analyze_with_selenium(url):
    """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–º–æ—â—å—é Selenium"""
    print(f"üîç –ê–Ω–∞–ª–∏–∑ —Å Selenium: {url}")
    
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1920,1080')
    
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        driver.set_page_load_timeout(30)
        
        driver.get(url)
        
        # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        time.sleep(5)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π HTML
        html = driver.page_source
        driver.quit()
        
        return html
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Selenium: {e}")
        if 'driver' in locals():
            driver.quit()
        return None

def extract_all_possible_data(html):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ HTML"""
    soup = BeautifulSoup(html, 'html.parser')
    
    print("\nüîç –ü–û–ò–°–ö –ù–ê–ó–í–ê–ù–ò–Ø –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø:")
    
    # –ü–æ–∏—Å–∫ –≤ title —Ç–µ–≥–∞—Ö
    title = soup.find('title')
    if title:
        print(f"   <title>: {title.get_text()[:100]}")
    
    # –ü–æ–∏—Å–∫ –≤–æ –≤—Å–µ—Ö h1 —Ç–µ–≥–∞—Ö
    h1_tags = soup.find_all('h1')
    for i, h1 in enumerate(h1_tags):
        text = h1.get_text().strip()
        if text and len(text) > 3:
            print(f"   H1 #{i+1}: {text[:100]}")
    
    # –ü–æ–∏—Å–∫ –≤–æ –≤—Å–µ—Ö h2 —Ç–µ–≥–∞—Ö
    h2_tags = soup.find_all('h2')
    for i, h2 in enumerate(h2_tags[:5]):  # –ü–µ—Ä–≤—ã–µ 5
        text = h2.get_text().strip()
        if text and len(text) > 3:
            print(f"   H2 #{i+1}: {text[:100]}")
    
    print("\nüîç –ü–û–ò–°–ö –†–ê–ó–†–ê–ë–û–¢–ß–ò–ö–ê:")
    
    # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö "By"
    by_elements = soup.find_all(string=re.compile(r'By\s+', re.IGNORECASE))
    for i, element in enumerate(by_elements[:10]):
        text = element.strip()
        if text and len(text) > 3:
            print(f"   By #{i+1}: {text[:100]}")
    
    # –ü–æ–∏—Å–∫ –≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞—Ö
    p_tags = soup.find_all('p')
    for i, p in enumerate(p_tags[:10]):
        text = p.get_text().strip()
        if 'by' in text.lower() and len(text) < 100:
            print(f"   P #{i+1}: {text}")
    
    print("\nüîç –ü–û–ò–°–ö –õ–û–ì–û–¢–ò–ü–û–í:")
    
    # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    img_tags = soup.find_all('img')
    for i, img in enumerate(img_tags[:15]):
        src = img.get('src', '')
        alt = img.get('alt', '')
        classes = ' '.join(img.get('class', []))
        if src and ('logo' in src.lower() or 'logo' in alt.lower() or 'logo' in classes.lower()):
            print(f"   IMG #{i+1}: src='{src[:100]}' alt='{alt}' class='{classes}'")
    
    print("\nüîç –ü–û–ò–°–ö –í JSON –î–ê–ù–ù–´–•:")
    
    # –ü–æ–∏—Å–∫ JSON –¥–∞–Ω–Ω—ã—Ö –≤ script —Ç–µ–≥–∞—Ö
    script_tags = soup.find_all('script')
    for i, script in enumerate(script_tags):
        text = script.get_text()
        if 'name' in text and 'publisher' in text and len(text) > 50:
            print(f"   SCRIPT #{i+1}: {text[:200]}...")
            
            # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å JSON
            json_patterns = [
                r'window\.__INITIAL_STATE__\s*=\s*({.+?});',
                r'window\.__APP_DATA__\s*=\s*({.+?});',
                r'__NEXT_DATA__\s*=\s*({.+?})',
                r'window\.pageData\s*=\s*({.+?});',
                r'"name"\s*:\s*"([^"]+)"',
                r'"publisher"\s*:\s*"([^"]+)"'
            ]
            
            for pattern in json_patterns:
                matches = re.findall(pattern, text, re.DOTALL)
                if matches:
                    print(f"      –ù–∞–π–¥–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω: {pattern[:50]}... -> {str(matches[:2])}")

def analyze_meta_tags(html):
    """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –º–µ—Ç–∞-—Ç–µ–≥–æ–≤"""
    soup = BeautifulSoup(html, 'html.parser')
    
    print("\nüîç –ê–ù–ê–õ–ò–ó –ú–ï–¢–ê-–¢–ï–ì–û–í:")
    
    # OpenGraph —Ç–µ–≥–∏
    og_tags = soup.find_all('meta', property=re.compile(r'^og:'))
    for tag in og_tags:
        prop = tag.get('property')
        content = tag.get('content', '')[:100]
        print(f"   {prop}: {content}")
    
    # Twitter —Ç–µ–≥–∏
    twitter_tags = soup.find_all('meta', name=re.compile(r'^twitter:'))
    for tag in twitter_tags:
        name = tag.get('name')
        content = tag.get('content', '')[:100]
        print(f"   {name}: {content}")
    
    # –î—Ä—É–≥–∏–µ –≤–∞–∂–Ω—ã–µ —Ç–µ–≥–∏
    important_tags = ['description', 'keywords', 'application-name']
    for tag_name in important_tags:
        tag = soup.find('meta', name=tag_name)
        if tag:
            content = tag.get('content', '')[:100]
            print(f"   {tag_name}: {content}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞"""
    test_url = 'https://appexchange.salesforce.com/appxListingDetail?listingId=a0N3000000B4cKBEAZ'
    
    print("=" * 80)
    print("üïµÔ∏è –ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ APPEXCHANGE –°–¢–†–ê–ù–ò–¶–´")
    print("=" * 80)
    
    # –ê–Ω–∞–ª–∏–∑ —Å Selenium
    html = analyze_with_selenium(test_url)
    
    if html:
        print(f"\n‚úÖ HTML –ø–æ–ª—É—á–µ–Ω, —Ä–∞–∑–º–µ—Ä: {len(html):,} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –°–æ—Ö—Ä–∞–Ω–∏–º HTML –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        with open('appexchange_page.html', 'w', encoding='utf-8') as f:
            f.write(html)
        print("üíæ HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ appexchange_page.html")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        extract_all_possible_data(html)
        analyze_meta_tags(html)
        
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML")

if __name__ == '__main__':
    main()