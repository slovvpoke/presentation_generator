"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä AppExchange —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
============================================================

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞, –¥–æ–±–∞–≤–ª—è—è:
- –ü–æ–¥–¥–µ—Ä–∂–∫—É –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ Selenium
- –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
- –õ—É—á—à—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫
"""

import os
import time
from typing import Optional, Tuple
from dataclasses import dataclass

import requests
from bs4 import BeautifulSoup
from PIL import Image
from io import BytesIO

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç Selenium
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from webdriver_manager.chrome import ChromeDriverManager
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

from sfapps_template_generator import AppMetadata


class EnhancedAppExchangeParser:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è AppExchange —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü"""
    
    def __init__(self, use_selenium=False, headless=True):
        self.use_selenium = use_selenium and SELENIUM_AVAILABLE
        self.headless = headless
        self.driver = None
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–æ–≤
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
    
    def __enter__(self):
        if self.use_selenium:
            self._setup_driver()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.driver:
            self.driver.quit()
    
    def _setup_driver(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Selenium WebDriver"""
        if not SELENIUM_AVAILABLE:
            raise ImportError("Selenium –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install selenium webdriver-manager")
        
        try:
            options = Options()
            if self.headless:
                options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--window-size=1920,1080')
            options.add_argument(f'--user-agent={self.headers["User-Agent"]}')
            
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            self.driver.set_page_load_timeout(30)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ WebDriver: {e}")
            self.driver = None
            self.use_selenium = False
    
    def fetch_page_content(self, url: str, timeout: int = 20) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        
        if self.use_selenium and self.driver:
            try:
                print(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Selenium –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏: {url}")
                self.driver.get(url)
                
                # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–∫—Ä—ã—Ç—å cookie –±–∞–Ω–Ω–µ—Ä
                try:
                    # –ò—â–µ–º –∫–Ω–æ–ø–∫–∏ –¥–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è cookie –±–∞–Ω–Ω–µ—Ä–∞
                    cookie_buttons = [
                        '[data-testid="cookie-accept"]',
                        '[data-testid="accept-cookies"]', 
                        'button[class*="cookie"]',
                        'button:contains("Accept")',
                        'button:contains("OK")',
                        '.cookie-banner button',
                        '#cookie-banner button'
                    ]
                    
                    for selector in cookie_buttons:
                        try:
                            button = WebDriverWait(self.driver, 2).until(
                                EC.element_to_be_clickable((By.CSS_SELECTOR, selector))
                            )
                            button.click()
                            print(f"–ó–∞–∫—Ä—ã—Ç cookie –±–∞–Ω–Ω–µ—Ä: {selector}")
                            time.sleep(1)
                            break
                        except:
                            continue
                except:
                    pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –µ—Å–ª–∏ cookie –±–∞–Ω–Ω–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                time.sleep(8)  # –£–≤–µ–ª–∏—á–∏–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è
                
                # –ü–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–æ–∫—Ä—É—Ç–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è —Ç—Ä–∏–≥–≥–µ—Ä–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                try:
                    self.driver.execute_script("window.scrollTo(0, 500);")
                    time.sleep(2)
                    self.driver.execute_script("window.scrollTo(0, 0);")
                    time.sleep(2)
                except:
                    pass
                
                return self.driver.page_source
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ Selenium: {e}, –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ requests")
        
        # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π HTTP –∑–∞–ø—Ä–æ—Å
        try:
            print(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ requests –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏: {url}")
            response = requests.get(url, headers=self.headers, timeout=timeout)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ HTTP –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return None
    
    def extract_app_metadata(self, html: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –∏–∑ HTML"""
        soup = BeautifulSoup(html, 'html.parser')
        name = None
        developer = None
        logo_url = None
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—ã (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è)
        name, developer, logo_url = self._extract_from_css_selectors(soup)
        print(f"CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—ã: name='{name}', developer='{developer}', logo_url='{logo_url}'")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: OpenGraph –∏ –º–µ—Ç–∞-—Ç–µ–≥–∏ (–¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ AppExchange —Å—Ç—Ä–∞–Ω–∏—Ü)
        if not all([name, developer, logo_url]):
            name2, developer2, logo_url2 = self._extract_from_meta_tags(soup)
            name = name or name2
            developer = developer or developer2
            logo_url = logo_url or logo_url2
            print(f"Meta —Ç–µ–≥–∏: name='{name}', developer='{developer}', logo_url='{logo_url}'")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ –≤ JSON –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏)
        if not all([name, developer, logo_url]):
            name3, developer3, logo_url3 = self._extract_from_json(soup)
            name = name or name3
            developer = developer or developer3
            logo_url = logo_url or logo_url3
            print(f"JSON –¥–∞–Ω–Ω—ã–µ: name='{name}', developer='{developer}', logo_url='{logo_url}'")
        
        return name, developer, logo_url
    
    def _extract_from_json(self, soup) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Å–∫—Ä–∏–ø—Ç–æ–≤"""
        import json
        import re
        
        name = None
        developer = None
        logo_url = None
        
        # –ü–æ–∏—Å–∫ –≤ script —Ç–µ–≥–∞—Ö —Å application/json
        script_tags = soup.find_all('script', type='application/json')
        for script in script_tags:
            try:
                data = json.loads(script.get_text())
                name, developer, logo_url = self._search_json_data(data)
                if name and developer:
                    break
            except:
                continue
        
        # –ü–æ–∏—Å–∫ –≤ –æ–±—ã—á–Ω—ã—Ö script —Ç–µ–≥–∞—Ö —Å JSON –¥–∞–Ω–Ω—ã–º–∏
        if not name:
            script_tags = soup.find_all('script')
            for script in script_tags:
                text = script.get_text()
                # –ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ JSON –≤ JavaScript
                json_patterns = [
                    r'window\.__INITIAL_STATE__\s*=\s*({.+?});',
                    r'window\.__APP_DATA__\s*=\s*({.+?});',
                    r'__NEXT_DATA__\s*=\s*({.+?})',
                    r'window\.pageData\s*=\s*({.+?});'
                ]
                
                for pattern in json_patterns:
                    match = re.search(pattern, text, re.DOTALL)
                    if match:
                        try:
                            data = json.loads(match.group(1))
                            name, developer, logo_url = self._search_json_data(data)
                            if name and developer:
                                break
                        except:
                            continue
                if name and developer:
                    break
        
        return name, developer, logo_url
    
    def _search_json_data(self, data, depth=0, max_depth=5):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä–µ"""
        if depth > max_depth or not isinstance(data, (dict, list)):
            return None, None, None
        
        if isinstance(data, dict):
            # –ü–æ–∏—Å–∫ –ø—Ä—è–º—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            if 'name' in data and 'publisher' in data:
                return data.get('name'), data.get('publisher'), data.get('logoUrl') or data.get('logo')
            
            if 'title' in data and 'developer' in data:
                return data.get('title'), data.get('developer'), data.get('imageUrl') or data.get('image')
            
            # –ü–æ–∏—Å–∫ –≤ –ø–æ–¥–æ–±—ä–µ–∫—Ç–∞—Ö
            for key, value in data.items():
                if isinstance(value, (dict, list)):
                    name, dev, logo = self._search_json_data(value, depth + 1, max_depth)
                    if name and dev:
                        return name, dev, logo
        
        elif isinstance(data, list):
            for item in data:
                name, dev, logo = self._search_json_data(item, depth + 1, max_depth)
                if name and dev:
                    return name, dev, logo
        
        return None, None, None
    
    def _extract_from_css_selectors(self, soup) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—ã"""
        name = None
        developer = None
        logo_url = None
        
        # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏—è (–±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ)
        name_selectors = [
            '.listing-title h1',
            'h1[type="style"]', 
            'main h1',
            '.app-title h1',
            'h1:not([class*="cookie"])',  # –ò—Å–∫–ª—é—á–∞–µ–º cookie —ç–ª–µ–º–µ–Ω—Ç—ã
            'h1'
        ]
        
        for selector in name_selectors:
            element = soup.select_one(selector)
            if element:
                text = element.get_text().strip()
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º cookie —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                if (text and 
                    len(text) > 5 and  # –ù–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ
                    'cookie' not in text.lower() and 
                    'privacy' not in text.lower()):
                    name = text
                    print(f"–ù–∞–π–¥–µ–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä '{selector}': {name}")
                    break
        
        # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ (–∏–∑–±–µ–≥–∞–µ–º cookie banner)
        dev_selectors = [
            '.listing-title p',  # –°–Ω–∞—á–∞–ª–∞ –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ
            'p[type="style"]',
            '.listing-header p',
            '.app-details p',
            'main p',  # –í –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            'div:not([class*="cookie"]) p'  # –ò—Å–∫–ª—é—á–∞–µ–º cookie —ç–ª–µ–º–µ–Ω—Ç—ã
        ]
        
        for selector in dev_selectors:
            element = soup.select_one(selector)
            if element:
                dev_text = element.get_text().strip()
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º cookie —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –ø—Ä–æ—á–∏–π –º—É—Å–æ—Ä
                if (dev_text and 
                    len(dev_text) < 200 and  # –ù–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                    'cookie' not in dev_text.lower() and 
                    'privacy' not in dev_text.lower() and
                    'statement' not in dev_text.lower()):
                    
                    if dev_text.lower().startswith('by '):
                        developer = dev_text[3:].strip()
                    else:
                        developer = dev_text
                    print(f"–ù–∞–π–¥–µ–Ω —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä '{selector}': {developer}")
                    break
        
        # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ª–æ–≥–æ—Ç–∏–ø–∞ (–ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ HTML)
        logo_selectors = [
            'img.ads-image',  # –¢–æ—á–Ω–æ –∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –≤ HTML —Å–ø—Ä–∞–≤–∞
            '.ads-image',
            '.listing-logo img',
            '.summary img',
            'img[class*="ads-image"]'
        ]
        
        for selector in logo_selectors:
            element = soup.select_one(selector)
            if element:
                logo_url = (element.get('src') or 
                           element.get('data-src') or 
                           element.get('data-original') or 
                           element.get('data-lazy') or
                           element.get('data-srcset', '').split(',')[0].strip().split(' ')[0])
                if logo_url:
                    print(f"–ù–∞–π–¥–µ–Ω –ª–æ–≥–æ—Ç–∏–ø —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä '{selector}': {logo_url}")
                    break
        
        return name, developer, logo_url
    
    def _extract_from_meta_tags(self, soup) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤"""
        name = None
        developer = None
        logo_url = None
        
        # OpenGraph –∑–∞–≥–æ–ª–æ–≤–æ–∫
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            title_content = og_title['content']
            if '|' in title_content:
                name = title_content.split('|')[0].strip()
            else:
                name = title_content.strip()
        
        # OpenGraph –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            logo_url = og_image['content']
        
        # Twitter –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞
        twitter_data1 = soup.find('meta', attrs={'name': 'twitter:data1'})
        if twitter_data1 and twitter_data1.get('content'):
            developer = twitter_data1['content'].strip()
        
        # –ü–æ–∏—Å–∫ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if not developer:
            description = soup.find('meta', attrs={'name': 'description'})
            if description and description.get('content'):
                desc_text = description['content']
                # –ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ "By Company Name"
                import re
                by_match = re.search(r'By\s+([^,\.\|]+)', desc_text, re.IGNORECASE)
                if by_match:
                    developer = by_match.group(1).strip()
        
        return name, developer, logo_url
    
    def fetch_app_metadata(self, url: str, timeout: int = 20) -> Optional[AppMetadata]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            html = self.fetch_page_content(url, timeout)
            if not html:
                return None
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            name, developer, logo_url = self.extract_app_metadata(html)
            
            if not name:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è")
                return None
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–≥–æ—Ç–∏–ø–∞
            logo_bytes = b''
            logo_mime = 'image/png'
            
            if logo_url:
                try:
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö URL
                    if logo_url.startswith('//'):
                        logo_url = 'https:' + logo_url
                    elif logo_url.startswith('/'):
                        from urllib.parse import urljoin, urlparse
                        base_url = f"{urlparse(url).scheme}://{urlparse(url).netloc}"
                        logo_url = urljoin(base_url, logo_url)
                    
                    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–≥–æ—Ç–∏–ø–∞: {logo_url}")
                    logo_response = requests.get(logo_url, headers=self.headers, timeout=10)
                    logo_response.raise_for_status()
                    
                    logo_bytes = logo_response.content
                    logo_mime = logo_response.headers.get('Content-Type', 'image/png')
                    
                    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    try:
                        with Image.open(BytesIO(logo_bytes)) as img:
                            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ (–∏–∑–±–µ–≥–∞–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
                            if img.width < 10 or img.height < 10:
                                print(f"–õ–æ–≥–æ—Ç–∏–ø —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π: {img.width}x{img.height}")
                                logo_bytes = b''
                    except Exception as e:
                        print(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
                        logo_bytes = b''
                        
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ª–æ–≥–æ—Ç–∏–ø–∞: {e}")
            
            return AppMetadata(
                url=url,
                name=name or 'Unknown App',
                developer=developer or 'Unknown Developer',
                logo_bytes=logo_bytes,
                logo_mime=logo_mime
            )
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
            return None


# –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–º–µ–Ω—ã –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π
def enhanced_fetch_app_metadata(url: str, timeout: int = 20, use_selenium: bool = False) -> Optional[AppMetadata]:
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è fetch_app_metadata —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    
    Parameters
    ----------
    url: str
        URL AppExchange —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    timeout: int
        –¢–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
    use_selenium: bool
        –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ Selenium –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    
    Returns
    -------
    AppMetadata or None
    """
    with EnhancedAppExchangeParser(use_selenium=use_selenium) as parser:
        return parser.fetch_app_metadata(url, timeout)


if __name__ == '__main__':
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
    test_urls = [
        'https://appexchange.salesforce.com/appxListingDetail?listingId=a0N3000000B4cKBEAZ',
        'https://appexchange.salesforce.com/appxListingDetail?listingId=a0N3u00000MSfukEAD'
    ]
    
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞")
    print("=" * 50)
    
    for i, url in enumerate(test_urls, 1):
        print(f"\nüìã –¢–µ—Å—Ç {i}: {url}")
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –±–µ–∑ Selenium
        print("\nüîÑ –¢–µ—Å—Ç –±–µ–∑ Selenium:")
        metadata = enhanced_fetch_app_metadata(url, use_selenium=False)
        if metadata:
            print(f"‚úÖ –ù–∞–∑–≤–∞–Ω–∏–µ: {metadata.name}")
            print(f"‚úÖ –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫: {metadata.developer}")
            print(f"‚úÖ –õ–æ–≥–æ—Ç–∏–ø: {len(metadata.logo_bytes)} –±–∞–π—Ç")
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ")
        
        # –ó–∞—Ç–µ–º –ø—Ä–æ–±—É–µ–º —Å Selenium (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        if SELENIUM_AVAILABLE:
            print("\nü§ñ –¢–µ—Å—Ç —Å Selenium:")
            metadata = enhanced_fetch_app_metadata(url, use_selenium=True)
            if metadata:
                print(f"‚úÖ –ù–∞–∑–≤–∞–Ω–∏–µ: {metadata.name}")
                print(f"‚úÖ –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫: {metadata.developer}")
                print(f"‚úÖ –õ–æ–≥–æ—Ç–∏–ø: {len(metadata.logo_bytes)} –±–∞–π—Ç")
            else:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ")
        else:
            print("\n‚ö†Ô∏è Selenium –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")